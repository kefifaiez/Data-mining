{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "L'objectif de cette séquence de code est d'effectuer une analyse lexicale complète sur un texte donné, en utilisant différentes techniques de traitement du langage naturel. Les objectifs spécifiques incluent la tokenization, la suppression de la ponctuation, la conversion en minuscules, la suppression des mots vides, la radicalisation et la lemmatisation des mots. L'objectif global est d'obtenir une représentation plus simplifiée et normalisée du texte, facilitant ainsi les tâches ultérieures telles que l'analyse de sentiments, la classification de texte ou la recherche d'informations"
      ],
      "metadata": {
        "id": "RyjVwBXBYqxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lecture et affichage du contenu d'un fichier texte avec Python"
      ],
      "metadata": {
        "id": "rxa-ErlCVgq2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5pPn341uJ6o",
        "outputId": "800786be-1f59-4bc5-c06a-5a933cb927e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Données lues à partir du fichier: Si vous vous intéressez au big data, vous connaissez certainement Apache Spark. Savez-vous pourquoi Spark est le framework de prédilection pour le traitement de données massives ? Pourquoi est-il auta\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Importer le module os pour travailler avec le système d'exploitation\n",
        "\n",
        "# Ouvrir le fichier \"Essentiel-Apache-Spark.txt\" en mode lecture à partir du chemin absolu du répertoire de travail\n",
        "with open(os.getcwd() + \"/Essentiel-Apache-Spark.txt\", 'r') as fh:\n",
        "    # Lire le contenu du fichier et le stocker dans la variable \"filedata\"\n",
        "    filedata = fh.read()\n",
        "\n",
        "# Afficher les 200 premiers caractères du contenu du fichier\n",
        "print(\"Données lues à partir du fichier:\", filedata[:200])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lecture et affichage du contenu d'un fichier texte en utilisant NLTK"
      ],
      "metadata": {
        "id": "UVk0co-xVuwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "\n",
        "#Lire le fichier dans un corpus . La mème commande peut lire un répertoire entier\n",
        "corpus = PlaintextCorpusReader(os.getcwd(), \"Essentiel-Apache-Spark.txt\")\n",
        "\n",
        "#afficher le contenu brut du corpus\n",
        "print(corpus.raw())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoR3wqTa0VfP",
        "outputId": "4a1cb2c3-6d77-4c04-bce8-8d94d86abc35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Si vous vous intéressez au big data, vous connaissez certainement Apache Spark. Savez-vous pourquoi Spark est le framework de prédilection pour le traitement de données massives ? Pourquoi est-il autant apprécié notamment pour déployer les algorithmes de machine learning ? Découvrez ce cours sur Apache PySpark pour répondre à toutes vos questions. À travers de multiples exemples et mises en pratique, le professeur associé en technologies de l'information et techniques d'optimisation, vous donne toutes les clés pour analyser efficacement des données à grande échelle avec Apache Spark et Python.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraction d'informations à partir d'un corpus texte avec NLTK"
      ],
      "metadata": {
        "id": "s_8P66NIV2SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the file IDs from the corpus.\n",
        "file_ids = corpus.fileids()\n",
        "print(\"Fichiers dans le corpus:\", file_ids)\n",
        "\n",
        "# Extract the paragraphs from the corpus.\n",
        "paragraphs = corpus.paras()\n",
        "print(\"\\n Nombre total de paragraphes dans le corpus:\", len(paragraphs))\n",
        "\n",
        "# Extract the sentences from the corpus.\n",
        "sentences = corpus.sents()\n",
        "print(\"\\n Nombre total de phrases dans le corpus:\", len(sentences))\n",
        "print(\"\\n Première phrase : \",sentences[0])\n",
        "\n",
        "# Extract the words from the corpus.\n",
        "words = corpus.words()\n",
        "print(\"\\n Nombre total de mots dans le corpus:\", len(words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tujZE8b-0Vl9",
        "outputId": "44851d0f-d573-4abd-c0e8-522d09a671fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fichiers dans le corpus: ['Essentiel-Apache-Spark.txt']\n",
            "\n",
            " Nombre total de paragraphes dans le corpus: 1\n",
            "\n",
            " Nombre total de phrases dans le corpus: 5\n",
            "\n",
            " Première phrase :  ['Si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', ',', 'vous', 'connaissez', 'certainement', 'Apache', 'Spark', '.']\n",
            "\n",
            " Nombre total de mots dans le corpus: 103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyse de la distribution des mots dans un corpus texte avec NLTK"
      ],
      "metadata": {
        "id": "MKa8BqS4V9xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Trouver la distrubition de fréquence des mots dans le corpus\n",
        "course_freq_dist=nltk.FreqDist(corpus.words())\n",
        "\n",
        "#afficher les mots les plus courament utilisés\n",
        "print(\"Top 10 des mots du corpus :\", course_freq_dist.most_common(10))\n",
        "\n",
        "#afficher les mots les plus courament utilisés\n",
        "print(\"\\n Distrubution pour le mots Spark :\", course_freq_dist.get(\"Spark\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xMG6Ht00Vpt",
        "outputId": "dfb3114e-5143-4be7-d20d-c9badd996fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 des mots du corpus : [('vous', 5), ('de', 5), ('pour', 4), (',', 3), ('Apache', 3), ('Spark', 3), ('.', 3), ('le', 3), ('et', 3), ('-', 2)]\n",
            "\n",
            " Distrubution pour le mots Spark : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization et comptage des mots dans un texte avec NLTK"
      ],
      "metadata": {
        "id": "ooLH8jNOWAqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "#télecharger le package punkt , utilisé une partie des autres comandes\n",
        "nltk.download('punkt')\n",
        "#Lire le fichier des base dans une variable de texte brut\n",
        "base_file= open(os.getcwd()+\"/Essentiel-Apache-Spark.txt\",'rt')\n",
        "raw_text=base_file.read()\n",
        "base_file.close()\n",
        "\n",
        "#Extraire des tokens\n",
        "token_list = nltk.word_tokenize(raw_text)\n",
        "print(\"Token List :\",token_list[:20])\n",
        "print(\"\\ Total Tokens : \",len(token_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCmxv92S0Vsq",
        "outputId": "f6406a84-6eca-4130-a0b5-40615b459157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token List : ['Si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', ',', 'vous', 'connaissez', 'certainement', 'Apache', 'Spark', '.', 'Savez-vous', 'pourquoi', 'Spark', 'est', 'le', 'framework']\n",
            "\\ Total Tokens :  95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppression de la ponctuation dans les tokens d'un texte avec NLTK"
      ],
      "metadata": {
        "id": "EeLqzzGOWMO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Télécharger le package 'punkt' de NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Utiliser la bibliothèque 'punkt' pour extraire les jetons sans ponctuation\n",
        "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
        "\n",
        "print(\"Liste des tokens après suppression de la ponctuation :\", token_list2[:20])\n",
        "print(\"Total des tokens après suppression de la ponctuation :\", len(token_list2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmEctP_Q0Vvc",
        "outputId": "54476d1d-72f4-4eb8-93d9-e54484138ae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liste des tokens après suppression de la ponctuation : ['Si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', 'vous', 'connaissez', 'certainement', 'Apache', 'Spark', 'Savez-vous', 'pourquoi', 'Spark', 'est', 'le', 'framework', 'de', 'prédilection']\n",
            "Total des tokens après suppression de la ponctuation : 87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversion des tokens en minuscules dans un texte avec Python"
      ],
      "metadata": {
        "id": "7ppn8vlhWTff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convertir en muniscules\n",
        "token_list3=[word.lower() for word in token_list2]\n",
        "print(\" Liste des tokens après conversion en miniscules :\",token_list3[:20])\n",
        "print(\"\\n Total des tokens après conversion en miniscules:\" , len(token_list3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsToxrLe0VyD",
        "outputId": "e3ff14be-03da-4b11-b847-07c39a8bf7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Liste des tokens après conversion en miniscules : ['si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', 'vous', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'est', 'le', 'framework', 'de', 'prédilection']\n",
            "\n",
            " Total des tokens après conversion en miniscules: 87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppression des mots vides dans un texte en français avec NLTK"
      ],
      "metadata": {
        "id": "6_KwUYFoWim-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Télécharger la liste standard des mots vides pour le français\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Obtenir la liste des mots vides pour le français\n",
        "stopwords_list = stopwords.words('french')\n",
        "\n",
        "# Supprimer les mots vides\n",
        "token_list4 = [token for token in token_list3 if token not in stopwords_list]\n",
        "\n",
        "print(\"Liste de tokens après suppression des mots vides :\", token_list4[:20])\n",
        "print(\"Total de tokens après suppression des mots vides :\", len(token_list4))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EZp2XMh0V0a",
        "outputId": "94195bb9-f744-48f8-afa2-f009b8627f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liste de tokens après suppression des mots vides : ['si', 'intéressez', 'big', 'data', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilection', 'traitement', 'données', 'massives', 'pourquoi', 'est-il', 'autant', 'apprécié']\n",
            "Total de tokens après suppression des mots vides : 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Radicalisation des mots dans un texte en français avec NLTK"
      ],
      "metadata": {
        "id": "1Wh7WUFoWyy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#utiliser la bibliotheque snowballStemmer pour la radicalisation\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer =SnowballStemmer(\"french\")\n",
        "#stem data\n",
        "token_list5= [stemmer.stem(word) for word in token_list4]\n",
        "print(\"Liste de tokens après le stemming : \",token_list5[:20])\n",
        "print(\"\\nTotal de tokens après Stemming : \",len(token_list5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDyryTg9yORa",
        "outputId": "6c6d8bce-41ac-4824-a010-8301364a7ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liste de tokens après le stemming :  ['si', 'intéress', 'big', 'dat', 'connaiss', 'certain', 'apach', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilect', 'trait', 'don', 'massiv', 'pourquoi', 'est-il', 'aut', 'appréci']\n",
            "\n",
            "Total de tokens après Stemming :  54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatisation des mots dans un texte avec NLTK"
      ],
      "metadata": {
        "id": "PEFik1zwXBTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Utiliser la bibliothèque wordnet pour mapper les mots à leur forme lemmatisée\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialiser le lemmatiseur\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatiser les tokens\n",
        "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4]\n",
        "\n",
        "print(\"Liste de tokens après lemmatisation :\", token_list6[:20])\n",
        "print(\"Total de tokens après lemmatisation :\", len(token_list6))\n"
      ],
      "metadata": {
        "id": "ikn74VHd0V3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6d4167-ec86-421b-80b8-c26de849a13b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Liste de tokens après lemmatisation : ['si', 'intéressez', 'big', 'data', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilection', 'traitement', 'données', 'massif', 'pourquoi', 'est-il', 'autant', 'apprécié']\n",
            "Total de tokens après lemmatisation : 54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation et téléchargement de ressources pour l'analyse linguistique en français avec spaCy"
      ],
      "metadata": {
        "id": "BgqUT1LaXV3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download fr_core_news_sm\n"
      ],
      "metadata": {
        "id": "g60U8zul2i5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partager l'analyse lexicale et la lemmatisation des tokens avec spaCy en français"
      ],
      "metadata": {
        "id": "BtjFYoBlXPv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Charger le modèle linguistique français\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "\n",
        "# Convertir la liste de tokens en une chaîne de caractères et l'analyser avec spaCy\n",
        "doc = nlp(\" \".join(token_list4))\n",
        "\n",
        "# Parcourir les tokens analysés et afficher le texte et la lemmatisation de chaque token\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5uO7we613ss",
        "outputId": "4ae96ec3-15e5-4951-b079-01a7f5293950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "si si\n",
            "intéressez intéresser\n",
            "big big\n",
            "data dater\n",
            "connaissez connaître\n",
            "certainement certainement\n",
            "apache apache\n",
            "spark spark\n",
            "savez savoir\n",
            "-vous vous\n",
            "pourquoi pourquoi\n",
            "spark spark\n",
            "framework framework\n",
            "prédilection prédilection\n",
            "traitement traitement\n",
            "données donnée\n",
            "massives massif\n",
            "pourquoi pourquoi\n",
            "est être\n",
            "-il il\n",
            "autant autant\n",
            "apprécié apprécier\n",
            "notamment notamment\n",
            "déployer déployer\n",
            "algorithmes algorithme\n",
            "machine machine\n",
            "learning learning\n",
            "découvrez découvrir\n",
            "cours cours\n",
            "apache apache\n",
            "pyspark pyspark\n",
            "répondre répondre\n",
            "toutes tout\n",
            "questions question\n",
            "travers travers\n",
            "multiples multiple\n",
            "exemples exemple\n",
            "mises mise\n",
            "pratique pratique\n",
            "professeur professeur\n",
            "associé associer\n",
            "technologies technologier\n",
            "l' le\n",
            "information information\n",
            "techniques technique\n",
            "d' de\n",
            "optimisation optimisation\n",
            "donne donne\n",
            "toutes tout\n",
            "clés clé\n",
            "analyser analyser\n",
            "efficacement efficacement\n",
            "données donnée\n",
            "grande grand\n",
            "échelle échelle\n",
            "apache apache\n",
            "spark spark\n",
            "python python\n"
          ]
        }
      ]
    }
  ]
}